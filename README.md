# Scratch-Implementation
Implementation of Neural Networks from scratch. Implementing Adam Optimization just by using numpy library, RMSProp, ReLU and it's derivatives are also implemented from scratch. Model is initialized with Xavier weight initializer. Training the model for 25 classes on alphabet-recognition dataset and noticing the trends of different epochs with learning rate.
